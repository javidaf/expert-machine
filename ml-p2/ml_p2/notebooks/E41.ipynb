{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain Gradient Descent with Fixed Learning Rate and Momentum\n",
    "\n",
    "We consider the Mean Squared Error (MSE) cost function for Ordinary Least Squares (OLS) regression:\n",
    "\n",
    "$$ C(\\beta) = \\frac{1}{n}(\\mathbf{X}\\beta - \\mathbf{y})^T(\\mathbf{X}\\beta - \\mathbf{y}) $$\n",
    "\n",
    "The analytical gradient is given by:\n",
    "\n",
    "$$ \\nabla C(\\beta) = \\frac{2}{n}\\mathbf{X}^T(\\mathbf{X}\\beta - \\mathbf{y}) $$\n",
    "\n",
    "The parameter update rule for plain gradient descent is:\n",
    "\n",
    "$$ \\beta = \\beta - \\eta \\nabla C(\\beta) $$\n",
    "\n",
    "where \\( \\eta \\) is the learning rate.\n",
    "\n",
    "To accelerate convergence, we can add **momentum** to the gradient descent algorithm. Momentum helps by considering the previous update's direction and magnitude, smoothing out the updates, and potentially leading to faster convergence.\n",
    "\n",
    "The momentum update equations are:\n",
    "\n",
    "$$ v_t = \\gamma v_{t-1} + \\eta \\nabla C(\\beta) $$\n",
    "$$ \\beta = \\beta - v_t $$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\( v_t \\) is the velocity vector at iteration \\( t \\).\n",
    "- \\( \\gamma \\) is the momentum coefficient (typically between 0 and 1).\n",
    "- \\( \\eta \\) is the learning rate.\n",
    "- \\( \\nabla C(\\beta) \\) is the gradient of the cost function.\n",
    "\n",
    "Our goal is to find the parameters \\( \\beta \\) that minimize the cost function using gradient descent with a fixed learning rate, and to compare the convergence with and without momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method Using the Hessian Matrix\n",
    "\n",
    "While gradient descent uses first-order derivatives to find the minimum of a function, Newton's method leverages second-order derivatives (the Hessian matrix) for potentially faster convergence.\n",
    "\n",
    "The update rule for Newton's method is:\n",
    "\n",
    "$$ \\beta = \\beta - H^{-1}(\\beta) \\nabla C(\\beta) $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\nabla C(\\beta)$ is the gradient vector.\n",
    "- $H(\\beta) $ is the Hessian matrix of second derivatives.\n",
    "\n",
    "For the MSE cost function in OLS regression, the Hessian matrix is given by:\n",
    "\n",
    "$$ H(\\beta) = \\frac{2}{n} \\mathbf{X}^T \\mathbf{X} $$\n",
    "\n",
    "Newton's method can achieve quadratic convergence near the optimum, but it requires computation of the Hessian and its inverse, which can be computationally intensive for large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
